{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare XLM-Roberta training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, XLMRobertaConfig, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model = \"xlm-roberta-base\"\n",
    "    max_len = 200\n",
    "    batch_size = 18\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=2e-5\n",
    "    num_warmup_steps = 0\n",
    "    use_scheduler = True\n",
    "    print_freq = 5000\n",
    "    debug = False\n",
    "    epochs = 5\n",
    "    scheduler = 'cosine'\n",
    "OUTPUT_DIR = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(predictions, th = 0.5):\n",
    "    preds = np.concatenate(predictions)\n",
    "    preds = np.where(preds>=0.5, 1, 0)\n",
    "    return preds\n",
    "\n",
    "def euclidianDistance(lat1,long1,lat2,long2):\n",
    "    return round(((lat2-lat1)**2+(long2-long1)**2)**(1/2), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/sentencepiece.bpe.model',\n",
       " './tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(CFG.model)\n",
    "CFG.tokenizer = tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.device = device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../folds/Fold_0_train.csv')\n",
    "test = pd.read_csv('../folds/Fold_0_test.csv')\n",
    "train = train.drop(columns=['poi1','poi2'])\n",
    "test = test.drop(columns=['poi1','poi2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    #train = pairsFull.iloc[:1000]\n",
    "    #test = pairsFull.iloc[1001:1100]\n",
    "    train = train.iloc[:100000]\n",
    "    test = test.iloc[100001:130000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna('unknown', inplace = True)\n",
    "test.fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match\n",
       "False    774993\n",
       "True     640370\n",
       "Name: id_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "match\n",
       "False    292315\n",
       "True     310133\n",
       "Name: id_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.groupby('match')['id_1'].count())\n",
    "display(test.groupby('match')['id_1'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match\n",
       "False    640370\n",
       "True     640370\n",
       "Name: id_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "match\n",
       "False    292315\n",
       "True     292315\n",
       "Name: id_1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# equalize match/nonmatch\n",
    "def equalizeMatches(df):\n",
    "    locations_of_matches = list(df[df['match'] == True].index)\n",
    "    locations_of_non_matches = list(df[df['match'] == False].index)\n",
    "    \n",
    "    if len(locations_of_matches) > len(locations_of_non_matches):\n",
    "        locations_of_matches = locations_of_matches[:len(locations_of_matches) - len(locations_of_non_matches)]\n",
    "        df = df.drop(locations_of_matches).reset_index(drop = True)\n",
    "    elif len(locations_of_matches) < len(locations_of_non_matches):\n",
    "        locations_of_non_matches = locations_of_non_matches[:len(locations_of_non_matches) - len(locations_of_matches)]\n",
    "        df = df.drop(locations_of_non_matches).reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "train = equalizeMatches(train)\n",
    "test = equalizeMatches(test)\n",
    "\n",
    "display(train.groupby('match')['id_1'].count())\n",
    "display(test.groupby('match')['id_1'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling data\n",
    "scaler = StandardScaler()\n",
    "num_features = [['latitude_1','longitude_1'],['latitude_2','longitude_2']]\n",
    "scaler.fit(train[num_features[0]])\n",
    "# save scaler for new data\n",
    "dump(scaler, 'std_scaler_main.bin', compress=True)\n",
    "train[num_features[0]] = scaler.transform(train[num_features[0]])\n",
    "train[num_features[1]] = scaler.transform(train[num_features[1]])\n",
    "test[num_features[0]] = scaler.transform(test[num_features[0]])\n",
    "test[num_features[1]] = scaler.transform(test[num_features[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate euclidian distance and add new feature\n",
    "train['distance'] = train.apply(lambda x: euclidianDistance(x.latitude_1,x.longitude_1,x.latitude_2,x.longitude_2), axis = 1)\n",
    "train = train.drop(columns = ['latitude_1','longitude_1','latitude_2','longitude_2'])\n",
    "\n",
    "test['distance'] = test.apply(lambda x: euclidianDistance(x.latitude_1,x.longitude_1,x.latitude_2,x.longitude_2), axis = 1)\n",
    "test = test.drop(columns = ['latitude_1','longitude_1','latitude_2','longitude_2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_1</th>\n",
       "      <th>name_1</th>\n",
       "      <th>categories_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>name_2</th>\n",
       "      <th>categories_2</th>\n",
       "      <th>match</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_594e4698d8d09b</td>\n",
       "      <td>Çırağan Sarayı</td>\n",
       "      <td>unknown</td>\n",
       "      <td>E_944324f1dcfe52</td>\n",
       "      <td>Çırağan Palace Kempinski Istanbul</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_b1fe6e102949c7</td>\n",
       "      <td>MK Live</td>\n",
       "      <td>Hotpot Restaurants, Chinese Restaurants</td>\n",
       "      <td>E_b7f2f3e748ee2a</td>\n",
       "      <td>MK Restaurant @ Central Festival Phuket</td>\n",
       "      <td>Diners</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_6ac370df3121eb</td>\n",
       "      <td>Ikea Alam Sutera Tangerang</td>\n",
       "      <td>unknown</td>\n",
       "      <td>E_8d3c176a5bf81a</td>\n",
       "      <td>IKEA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_50c2f4fbe37487</td>\n",
       "      <td>Morrison Dental</td>\n",
       "      <td>Dentist's Offices</td>\n",
       "      <td>E_aa31f5f5be7994</td>\n",
       "      <td>Morrison Dental Care</td>\n",
       "      <td>Dentist's Offices</td>\n",
       "      <td>True</td>\n",
       "      <td>0.033926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_0c12d1ffe7f5eb</td>\n",
       "      <td>Baskin-Robbins</td>\n",
       "      <td>Ice Cream Shops</td>\n",
       "      <td>E_a18a2a08db2c6c</td>\n",
       "      <td>Baskin Robbins</td>\n",
       "      <td>Miscellaneous Shops</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id_1                      name_1  \\\n",
       "0  E_594e4698d8d09b              Çırağan Sarayı   \n",
       "1  E_b1fe6e102949c7                     MK Live   \n",
       "2  E_6ac370df3121eb  Ikea Alam Sutera Tangerang   \n",
       "3  E_50c2f4fbe37487             Morrison Dental   \n",
       "4  E_0c12d1ffe7f5eb              Baskin-Robbins   \n",
       "\n",
       "                              categories_1              id_2  \\\n",
       "0                                  unknown  E_944324f1dcfe52   \n",
       "1  Hotpot Restaurants, Chinese Restaurants  E_b7f2f3e748ee2a   \n",
       "2                                  unknown  E_8d3c176a5bf81a   \n",
       "3                        Dentist's Offices  E_aa31f5f5be7994   \n",
       "4                          Ice Cream Shops  E_a18a2a08db2c6c   \n",
       "\n",
       "                                    name_2         categories_2  match  \\\n",
       "0        Çırağan Palace Kempinski Istanbul               Hotels   True   \n",
       "1  MK Restaurant @ Central Festival Phuket               Diners   True   \n",
       "2                                     IKEA              unknown   True   \n",
       "3                     Morrison Dental Care    Dentist's Offices   True   \n",
       "4                           Baskin Robbins  Miscellaneous Shops   True   \n",
       "\n",
       "   distance  \n",
       "0  0.000025  \n",
       "1  0.000010  \n",
       "2  0.002272  \n",
       "3  0.033926  \n",
       "4  0.000011  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepareInputs(line):\n",
    "    text = str(line['name_1']) + '[SEP]' + str(line['categories_1']) + '[SEP]' + str(line['distance']) \\\n",
    "    + '[SEP]'+ str(line['name_2']) + '[SEP]'+ str(line['categories_2'])\n",
    "    \n",
    "    inputs = CFG.tokenizer(text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    \n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "def prepareLabels(value):\n",
    "    if value == True:\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "class PairsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "        self.labels = pairs['match'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = prepareInputs(self.pairs.iloc[idx])\n",
    "        label = prepareLabels(self.labels[idx])\n",
    "        \n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if config_path is None:\n",
    "            self.config = XLMRobertaConfig.from_pretrained(CFG.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = XLMRobertaModel.from_pretrained(CFG.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "            \n",
    "\n",
    "        self.model = XLMRobertaModel.from_pretrained(CFG.model, config=self.config)\n",
    "        \n",
    "        self.fc_dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state[:,0,:]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))        \n",
    "        \n",
    "        \n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1,1),labels.view(-1,1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if CFG.use_scheduler:\n",
    "            scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        end = time.time()\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainigLoop(train,test, epochs):\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_dataset = PairsDataset(train)\n",
    "    test_dataset = PairsDataset(test)\n",
    "    test_labels = np.where(test['match'].to_numpy() == True, 1,0)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, pin_memory = True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, pin_memory = True)\n",
    "    \n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    tokenizer = CFG.tokenizer\n",
    "    model = CustomModel(pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(CFG.device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.encoder_lr)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    \n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_score = 0.\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(train_dataloader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n",
    "        \n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(test_dataloader, model, criterion, CFG.device)\n",
    "        results = get_results(predictions, th = 0.5)\n",
    "        score = f1_score(test_labels,results)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            print(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_best.pth\")\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_best.pth\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\obucheniye\\anaconda_new\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/71153] Elapsed 0m 11s (remain 13115m 1s) Loss: 0.6715(0.6715) LR: 0.00002000  \n",
      "Epoch: [1][5000/71153] Elapsed 32m 15s (remain 426m 37s) Loss: 0.0603(0.2370) LR: 0.00001999  \n",
      "Epoch: [1][10000/71153] Elapsed 64m 19s (remain 393m 19s) Loss: 0.2160(0.2131) LR: 0.00001996  \n",
      "Epoch: [1][15000/71153] Elapsed 96m 23s (remain 360m 50s) Loss: 0.1270(0.2000) LR: 0.00001991  \n",
      "Epoch: [1][20000/71153] Elapsed 128m 27s (remain 328m 32s) Loss: 0.0243(0.1916) LR: 0.00001984  \n",
      "Epoch: [1][25000/71153] Elapsed 160m 31s (remain 296m 20s) Loss: 0.0376(0.1855) LR: 0.00001976  \n",
      "Epoch: [1][30000/71153] Elapsed 192m 57s (remain 264m 40s) Loss: 0.0461(0.1800) LR: 0.00001965  \n",
      "Epoch: [1][35000/71153] Elapsed 225m 1s (remain 232m 24s) Loss: 0.0529(0.1759) LR: 0.00001953  \n",
      "Epoch: [1][40000/71153] Elapsed 257m 4s (remain 200m 12s) Loss: 0.0141(0.1725) LR: 0.00001938  \n",
      "Epoch: [1][45000/71153] Elapsed 289m 8s (remain 168m 1s) Loss: 0.1082(0.1696) LR: 0.00001922  \n",
      "Epoch: [1][50000/71153] Elapsed 321m 12s (remain 135m 52s) Loss: 0.2348(0.1671) LR: 0.00001904  \n",
      "Epoch: [1][55000/71153] Elapsed 353m 15s (remain 103m 44s) Loss: 0.0433(0.1645) LR: 0.00001884  \n",
      "Epoch: [1][60000/71153] Elapsed 385m 23s (remain 71m 37s) Loss: 0.0290(0.1621) LR: 0.00001863  \n",
      "Epoch: [1][65000/71153] Elapsed 417m 28s (remain 39m 30s) Loss: 0.1210(0.1603) LR: 0.00001840  \n",
      "Epoch: [1][70000/71153] Elapsed 449m 34s (remain 7m 23s) Loss: 0.1143(0.1581) LR: 0.00001815  \n",
      "Epoch: [1][71152/71153] Elapsed 457m 10s (remain 0m 0s) Loss: 0.0007(0.1577) LR: 0.00001809  \n",
      "EVAL: [0/32480] Elapsed 0m 0s (remain 105m 44s) Loss: 0.1033(0.1033) \n",
      "EVAL: [5000/32480] Elapsed 10m 0s (remain 55m 0s) Loss: 0.2713(0.1539) \n",
      "EVAL: [10000/32480] Elapsed 19m 58s (remain 44m 52s) Loss: 0.4101(0.1636) \n",
      "EVAL: [15000/32480] Elapsed 29m 54s (remain 34m 50s) Loss: 0.0865(0.1692) \n",
      "EVAL: [20000/32480] Elapsed 39m 51s (remain 24m 51s) Loss: 0.0505(0.1726) \n",
      "EVAL: [25000/32480] Elapsed 49m 47s (remain 14m 53s) Loss: 0.0541(0.1735) \n",
      "EVAL: [30000/32480] Elapsed 59m 45s (remain 4m 56s) Loss: 0.1447(0.1753) \n",
      "EVAL: [32479/32480] Elapsed 64m 40s (remain 0m 0s) Loss: 0.1663(0.1760) \n",
      "Epoch 1 - avg_train_loss: 0.1577  avg_val_loss: 0.1760  time: 31312s\n",
      "Epoch 1 - Score: 0.9391\n",
      "Epoch 1 - Save Best Score: 0.9391 Model\n",
      "./xlm-roberta-base_best.pth\n",
      "Epoch: [2][0/71153] Elapsed 0m 0s (remain 797m 53s) Loss: 0.1299(0.1299) LR: 0.00001809  \n",
      "Epoch: [2][5000/71153] Elapsed 32m 3s (remain 424m 8s) Loss: 0.0612(0.1195) LR: 0.00001782  \n",
      "Epoch: [2][10000/71153] Elapsed 64m 17s (remain 393m 5s) Loss: 0.0704(0.1190) LR: 0.00001754  \n",
      "Epoch: [2][15000/71153] Elapsed 96m 39s (remain 361m 47s) Loss: 0.0899(0.1176) LR: 0.00001724  \n",
      "Epoch: [2][20000/71153] Elapsed 128m 43s (remain 329m 12s) Loss: 0.0729(0.1166) LR: 0.00001693  \n",
      "Epoch: [2][25000/71153] Elapsed 160m 48s (remain 296m 50s) Loss: 0.1128(0.1163) LR: 0.00001661  \n",
      "Epoch: [2][30000/71153] Elapsed 192m 52s (remain 264m 34s) Loss: 0.0728(0.1157) LR: 0.00001627  \n",
      "Epoch: [2][35000/71153] Elapsed 224m 58s (remain 232m 21s) Loss: 0.0185(0.1153) LR: 0.00001592  \n",
      "Epoch: [2][40000/71153] Elapsed 257m 2s (remain 200m 10s) Loss: 0.2589(0.1149) LR: 0.00001556  \n",
      "Epoch: [2][45000/71153] Elapsed 289m 2s (remain 167m 58s) Loss: 0.0504(0.1143) LR: 0.00001518  \n",
      "Epoch: [2][50000/71153] Elapsed 321m 0s (remain 135m 47s) Loss: 0.2269(0.1137) LR: 0.00001480  \n",
      "Epoch: [2][55000/71153] Elapsed 353m 2s (remain 103m 40s) Loss: 0.0639(0.1131) LR: 0.00001441  \n",
      "Epoch: [2][60000/71153] Elapsed 385m 6s (remain 71m 34s) Loss: 0.1592(0.1126) LR: 0.00001401  \n",
      "Epoch: [2][65000/71153] Elapsed 417m 10s (remain 39m 29s) Loss: 0.0065(0.1120) LR: 0.00001360  \n",
      "Epoch: [2][70000/71153] Elapsed 449m 15s (remain 7m 23s) Loss: 0.0609(0.1115) LR: 0.00001319  \n",
      "Epoch: [2][71152/71153] Elapsed 456m 38s (remain 0m 0s) Loss: 0.0181(0.1114) LR: 0.00001309  \n",
      "EVAL: [0/32480] Elapsed 0m 0s (remain 98m 0s) Loss: 0.1263(0.1263) \n",
      "EVAL: [5000/32480] Elapsed 9m 58s (remain 54m 46s) Loss: 0.2048(0.1460) \n",
      "EVAL: [10000/32480] Elapsed 19m 57s (remain 44m 51s) Loss: 0.2869(0.1557) \n",
      "EVAL: [15000/32480] Elapsed 29m 57s (remain 34m 53s) Loss: 0.0321(0.1615) \n",
      "EVAL: [20000/32480] Elapsed 39m 56s (remain 24m 55s) Loss: 0.0192(0.1651) \n",
      "EVAL: [25000/32480] Elapsed 49m 54s (remain 14m 55s) Loss: 0.0223(0.1662) \n",
      "EVAL: [30000/32480] Elapsed 59m 53s (remain 4m 56s) Loss: 0.2177(0.1683) \n",
      "EVAL: [32479/32480] Elapsed 64m 51s (remain 0m 0s) Loss: 0.0948(0.1689) \n",
      "Epoch 2 - avg_train_loss: 0.1114  avg_val_loss: 0.1689  time: 31291s\n",
      "Epoch 2 - Score: 0.9411\n",
      "Epoch 2 - Save Best Score: 0.9411 Model\n",
      "./xlm-roberta-base_best.pth\n",
      "Epoch: [3][0/71153] Elapsed 0m 0s (remain 712m 41s) Loss: 0.0341(0.0341) LR: 0.00001309  \n",
      "Epoch: [3][5000/71153] Elapsed 32m 13s (remain 426m 16s) Loss: 0.0108(0.0873) LR: 0.00001267  \n",
      "Epoch: [3][10000/71153] Elapsed 64m 26s (remain 394m 1s) Loss: 0.1176(0.0885) LR: 0.00001224  \n",
      "Epoch: [3][15000/71153] Elapsed 96m 35s (remain 361m 33s) Loss: 0.0243(0.0879) LR: 0.00001181  \n",
      "Epoch: [3][20000/71153] Elapsed 128m 40s (remain 329m 4s) Loss: 0.1092(0.0886) LR: 0.00001137  \n",
      "Epoch: [3][25000/71153] Elapsed 160m 46s (remain 296m 48s) Loss: 0.0343(0.0878) LR: 0.00001093  \n",
      "Epoch: [3][30000/71153] Elapsed 192m 51s (remain 264m 33s) Loss: 0.0178(0.0880) LR: 0.00001049  \n",
      "Epoch: [3][35000/71153] Elapsed 224m 56s (remain 232m 20s) Loss: 0.1035(0.0872) LR: 0.00001005  \n",
      "Epoch: [3][40000/71153] Elapsed 257m 1s (remain 200m 9s) Loss: 0.0247(0.0866) LR: 0.00000961  \n",
      "Epoch: [3][45000/71153] Elapsed 289m 6s (remain 168m 0s) Loss: 0.0221(0.0863) LR: 0.00000917  \n",
      "Epoch: [3][50000/71153] Elapsed 321m 10s (remain 135m 52s) Loss: 0.0240(0.0859) LR: 0.00000873  \n",
      "Epoch: [3][55000/71153] Elapsed 353m 15s (remain 103m 44s) Loss: 0.0730(0.0854) LR: 0.00000829  \n",
      "Epoch: [3][60000/71153] Elapsed 385m 20s (remain 71m 37s) Loss: 0.1036(0.0849) LR: 0.00000786  \n",
      "Epoch: [3][65000/71153] Elapsed 417m 24s (remain 39m 30s) Loss: 0.2435(0.0846) LR: 0.00000743  \n",
      "Epoch: [3][70000/71153] Elapsed 449m 29s (remain 7m 23s) Loss: 0.1759(0.0842) LR: 0.00000701  \n",
      "Epoch: [3][71152/71153] Elapsed 456m 52s (remain 0m 0s) Loss: 0.0016(0.0841) LR: 0.00000691  \n",
      "EVAL: [0/32480] Elapsed 0m 0s (remain 96m 55s) Loss: 0.1395(0.1395) \n",
      "EVAL: [5000/32480] Elapsed 9m 59s (remain 54m 52s) Loss: 0.2957(0.1681) \n",
      "EVAL: [10000/32480] Elapsed 20m 0s (remain 44m 58s) Loss: 0.4116(0.1768) \n",
      "EVAL: [15000/32480] Elapsed 29m 57s (remain 34m 54s) Loss: 0.0254(0.1821) \n",
      "EVAL: [20000/32480] Elapsed 39m 54s (remain 24m 54s) Loss: 0.0175(0.1859) \n",
      "EVAL: [25000/32480] Elapsed 49m 51s (remain 14m 55s) Loss: 0.0140(0.1865) \n",
      "EVAL: [30000/32480] Elapsed 59m 49s (remain 4m 56s) Loss: 0.2816(0.1885) \n",
      "EVAL: [32479/32480] Elapsed 64m 46s (remain 0m 0s) Loss: 0.0131(0.1891) \n",
      "Epoch 3 - avg_train_loss: 0.0841  avg_val_loss: 0.1891  time: 31299s\n",
      "Epoch 3 - Score: 0.9428\n",
      "Epoch 3 - Save Best Score: 0.9428 Model\n",
      "./xlm-roberta-base_best.pth\n",
      "Epoch: [4][0/71153] Elapsed 0m 0s (remain 788m 45s) Loss: 0.0205(0.0205) LR: 0.00000691  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-627b499ed576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrainigLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-26d90166c91f>\u001b[0m in \u001b[0;36mtrainigLoop\u001b[1;34m(train, test, epochs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-b7d1b126c132>\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_loader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mCFG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_scheduler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\obucheniye\\anaconda_new\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\obucheniye\\anaconda_new\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    trainigLoop(train,test, CFG.epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
