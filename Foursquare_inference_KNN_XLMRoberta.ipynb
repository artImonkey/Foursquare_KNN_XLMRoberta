{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference KNN + XLMRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, XLMRobertaConfig, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    model = \"xlm-roberta-base\"\n",
    "    max_len = 200\n",
    "    batch_size = 500\n",
    "    num_warmup_steps = 0\n",
    "    use_scheduler = True\n",
    "    print_freq = 100\n",
    "    debug = True\n",
    "    epochs = 5\n",
    "    config_path = 'config.pth'\n",
    "    model_path = 'xlm-roberta-base_best.pth'\n",
    "    tokenizer_path = 'tokenizer/'\n",
    "    final = False\n",
    "    scaler_2 = 'std_scaler_main.bin'\n",
    "    th = 0.5\n",
    "    \n",
    "OUTPUT_DIR = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>categories</th>\n",
       "      <th>point_of_interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_22e8722af27f8f</td>\n",
       "      <td>Tongatapu International Airport, Tongatapu, Tonga</td>\n",
       "      <td>-21.139430</td>\n",
       "      <td>-175.160248</td>\n",
       "      <td>Airports</td>\n",
       "      <td>P_94d933e731ebf7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_77bd2381786179</td>\n",
       "      <td>Fuaʻamotu International Airport</td>\n",
       "      <td>-21.241393</td>\n",
       "      <td>-175.141634</td>\n",
       "      <td>Airports</td>\n",
       "      <td>P_94d933e731ebf7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_4b77ccd07a7889</td>\n",
       "      <td>Fua'amotu International Airport (TBU)</td>\n",
       "      <td>-21.244104</td>\n",
       "      <td>-175.137096</td>\n",
       "      <td>Airports</td>\n",
       "      <td>P_94d933e731ebf7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_2088cfef49b8c5</td>\n",
       "      <td>Janes Beach Fales</td>\n",
       "      <td>-13.447485</td>\n",
       "      <td>-172.376063</td>\n",
       "      <td>Resorts</td>\n",
       "      <td>P_85041d895cbb1f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_4515f166a43037</td>\n",
       "      <td>Etelinas Pizzeria</td>\n",
       "      <td>-13.442352</td>\n",
       "      <td>-172.358032</td>\n",
       "      <td>Pizza Places</td>\n",
       "      <td>P_8cc6a008838677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               name  \\\n",
       "0  E_22e8722af27f8f  Tongatapu International Airport, Tongatapu, Tonga   \n",
       "1  E_77bd2381786179                    Fuaʻamotu International Airport   \n",
       "2  E_4b77ccd07a7889              Fua'amotu International Airport (TBU)   \n",
       "3  E_2088cfef49b8c5                                  Janes Beach Fales   \n",
       "4  E_4515f166a43037                                  Etelinas Pizzeria   \n",
       "\n",
       "    latitude   longitude    categories point_of_interest  \n",
       "0 -21.139430 -175.160248      Airports  P_94d933e731ebf7  \n",
       "1 -21.241393 -175.141634      Airports  P_94d933e731ebf7  \n",
       "2 -21.244104 -175.137096      Airports  P_94d933e731ebf7  \n",
       "3 -13.447485 -172.376063       Resorts  P_85041d895cbb1f  \n",
       "4 -13.442352 -172.358032  Pizza Places  P_8cc6a008838677  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import train dataset\n",
    "init_data = pd.read_csv('datasets/train.csv')\n",
    "init_data = init_data.drop(columns=['address','city','state','zip','country','url','phone'])\n",
    "init_data = init_data.sort_values(by = ['longitude']).reset_index(drop=True)\n",
    "init_data.fillna('unknown', inplace = True)\n",
    "init_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug == True:\n",
    "    init_data = init_data.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.final == False:\n",
    "    init_data = init_data.drop(columns = ['point_of_interest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(predictions, th = 0.5):\n",
    "    preds = np.concatenate(predictions)\n",
    "    preds = np.where(preds>=th, 1, 0)\n",
    "    return preds\n",
    "def euclidianDistance(lat1,long1,lat2,long2):\n",
    "    return round(((lat2-lat1)**2+(long2-long1)**2)**(1/2), 6)\n",
    "\n",
    "def JaccartCoef(set1,set2):\n",
    "    intesect = set1.intersection(set2)\n",
    "    return len(intesect)/(len(set1)+len(set2) - len(intesect))\n",
    "\n",
    "    \n",
    "def intersectionOverUnion(train_scoring_dict,predicted_df):\n",
    "    coefs = []\n",
    "    predicted_ids = set(predicted_df['id'].values)\n",
    "    for _,predicted_row in predicted_df.iterrows():\n",
    "        predicted_matches = set(predicted_row['match'].split(' '))\n",
    "        train_matches = set()\n",
    "        for id in train_scoring_dict[predicted_row['id']]:\n",
    "            if id in predicted_ids:\n",
    "                train_matches.add(id)\n",
    "        coefs.append(JaccartCoef(train_matches,predicted_matches))          \n",
    "    return sum(coefs)/len(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E_22e8722af27f8f</td>\n",
       "      <td>E_22e8722af27f8f E_77bd2381786179 E_4b77ccd07a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E_77bd2381786179</td>\n",
       "      <td>E_22e8722af27f8f E_77bd2381786179 E_4b77ccd07a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E_4b77ccd07a7889</td>\n",
       "      <td>E_22e8722af27f8f E_77bd2381786179 E_4b77ccd07a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E_2088cfef49b8c5</td>\n",
       "      <td>E_2088cfef49b8c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E_4515f166a43037</td>\n",
       "      <td>E_4515f166a43037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                            matches\n",
       "0  E_22e8722af27f8f  E_22e8722af27f8f E_77bd2381786179 E_4b77ccd07a...\n",
       "1  E_77bd2381786179  E_22e8722af27f8f E_77bd2381786179 E_4b77ccd07a...\n",
       "2  E_4b77ccd07a7889  E_22e8722af27f8f E_77bd2381786179 E_4b77ccd07a...\n",
       "3  E_2088cfef49b8c5                                   E_2088cfef49b8c5\n",
       "4  E_4515f166a43037                                   E_4515f166a43037"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dict for scoring\n",
    "answers_df = pd.read_csv('train_df_for_scoring.csv')\n",
    "train_scoring_dict = answers_df.set_index('id').to_dict()['matches']\n",
    "answers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in train_scoring_dict:\n",
    "    train_scoring_dict[ind] = set(train_scoring_dict[ind].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E_22e8722af27f8f', 'E_4b77ccd07a7889', 'E_77bd2381786179'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scoring_dict['E_22e8722af27f8f']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(CFG.tokenizer_path)\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.device = device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepareInputs(line):\n",
    "    text = str(line['name_1']) + '[SEP]' + str(line['categories_1']) + '[SEP]' + str(line['distance']) \\\n",
    "    + '[SEP]'+ str(line['name_2']) + '[SEP]'+ str(line['categories_2'])\n",
    "    \n",
    "    inputs = CFG.tokenizer(text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    \n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "class PairsDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = prepareInputs(self.pairs.iloc[idx])\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(CFG.model, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(CFG.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "            \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.config = XLMRobertaConfig.from_pretrained(CFG.model, output_hidden_states=True)\n",
    "        self.model = XLMRobertaModel.from_pretrained(CFG.model, config=self.config)'''\n",
    "        #self.config = torch.load(config_path)\n",
    "        #self.model = AutoModel(self.config) \n",
    "        #self.model = AutoModel.from_config(self.config)\n",
    "        \n",
    "        self.fc_dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state[:,0,:]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))   \n",
    "\n",
    "def infer_fn(infer_loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, inputs in enumerate(infer_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(infer_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(step, len(infer_loader),\n",
    "                          remain=timeSince(start, float(step+1)/len(infer_loader))))\n",
    "    \n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferenceMain(dataset):\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    infer_dataset = PairsDataset(dataset)\n",
    "    \n",
    "    infer_dataloader = DataLoader(infer_dataset, batch_size=CFG.batch_size, shuffle=False, pin_memory = True)\n",
    "\n",
    "    #infer_labels = np.where(test['match'].to_numpy() == True, 1,0)\n",
    "    # ====================================================\n",
    "    # model & predictioning\n",
    "    # ====================================================\n",
    "    tokenizer = CFG.tokenizer\n",
    "    model = CustomModel(config_path = CFG.config_path, pretrained=False)\n",
    "    state = torch.load(CFG.model_path,\n",
    "                       map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictions = infer_fn(infer_dataloader, model, device)\n",
    "\n",
    "    results = np.concatenate(predictions)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'time: {elapsed:.0f}s')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing with bunches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePairsIds(init_data, n_neighbours = 7 ):\n",
    "    #import scaler and scale\n",
    "    data = init_data.copy()\n",
    "    scaler= StandardScaler()\n",
    "    num_features = ['latitude','longitude']\n",
    "    data[num_features] = scaler.fit_transform(data[num_features])\n",
    "    \n",
    "    X = data[['latitude','longitude']]\n",
    "    \n",
    "    # apply KNN\n",
    "    nbrs = NearestNeighbors(n_neighbors = n_neighbours, algorithm='kd_tree').fit(X)\n",
    "    indices = nbrs.kneighbors(X,return_distance=False)\n",
    "    indices_set = list(map(set,indices))\n",
    "    \n",
    "    # remove any duplicated pairs: if we have a=b, delete b=a\n",
    "    for primary_id in tqdm(range(len(indices_set))):\n",
    "        ind_set = indices_set[primary_id]\n",
    "        if primary_id in ind_set:\n",
    "            ind_set.remove(primary_id)\n",
    "\n",
    "        for secondary_id in ind_set:\n",
    "            if primary_id in indices_set[secondary_id]:\n",
    "                indices_set[secondary_id].remove(primary_id)\n",
    "    count = 0\n",
    "    for i in indices_set:\n",
    "        count += len(i)\n",
    "    print(f'{count} pairs ids generated')\n",
    "    return indices_set\n",
    "\n",
    "def fillIdDict(matches, id_dict):\n",
    "    start_time = time.time()\n",
    "    for i in tqdm(range(len(matches))):\n",
    "        row = matches.iloc[i]\n",
    "        id1 = row['id_1']\n",
    "        id2 = row['id_2']\n",
    "        similarity = row['match']\n",
    "        id_dict[id1].neighbours[id2] = similarity\n",
    "        id_dict[id2].neighbours[id1] = similarity\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'ID dict generated. Time elapsed: {elapsed:.0} s')\n",
    "    return id_dict\n",
    "\n",
    "class EntryID:\n",
    "    # class is not justified, could be replaced with just dict\n",
    "    def __init__(self, index):\n",
    "        self.neighbours = {index: 1}\n",
    "        \n",
    "def findMoreNeighbours(weight, primary_id, id_dict, current_id, visited, th):\n",
    "    id_obj = id_dict[current_id]\n",
    "    result = {}\n",
    "    if current_id not in visited:\n",
    "        visited.add(current_id)\n",
    "        for key in id_obj.neighbours:\n",
    "            summary_prob = id_obj.neighbours[key]*weight\n",
    "            if  summary_prob >= th and key != primary_id and key != current_id:\n",
    "                result[key] = summary_prob\n",
    "                current_result = findMoreNeighbours(summary_prob, primary_id, id_dict, key, visited, th)\n",
    "                result.update(current_result)\n",
    "                \n",
    "    return result\n",
    "    \n",
    "\n",
    "def postProcess(id_dict, th = 0.5):\n",
    "    # the idea is if we have results as probabilities, we could \n",
    "    # connect entries A and B that are not neighbours if their  \n",
    "    # product is bigger than 'th'\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for entry_id in id_dict:\n",
    "        neighbours = list(id_dict[entry_id].neighbours.keys()).copy()\n",
    "        for current_id in neighbours:\n",
    "            weight = id_dict[entry_id].neighbours[current_id]\n",
    "            if  weight> th and current_id != entry_id:\n",
    "                result = findMoreNeighbours(weight, entry_id, id_dict, current_id, set(), th)\n",
    "                \n",
    "                neighbours_dict = id_dict[entry_id].neighbours\n",
    "                for key in result:\n",
    "                    score = neighbours_dict.get(key)\n",
    "                    if not score:\n",
    "                        neighbours_dict[key] = result[key]\n",
    "                    elif score < result[key]:\n",
    "                        neighbours_dict[key] = result[key]\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'Post processing finished. Time elapsed: {elapsed}')\n",
    "    return id_dict\n",
    "        \n",
    "def pairsGenerator(df, ids):\n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "    ids1 = []\n",
    "    ids2 = []\n",
    "    for entry_id in range(len(ids)):\n",
    "        if len(ids[entry_id]) > 0:\n",
    "            current_ids2 = list(ids[entry_id])\n",
    "            current_ids1 = [entry_id for _ in range(len(current_ids2))]\n",
    "            \n",
    "            ids1.extend(current_ids1)\n",
    "            ids2.extend(current_ids2)\n",
    "            \n",
    "            count += len(current_ids2)\n",
    "\n",
    "            if count > 500000:\n",
    "                match_to_add = pd.concat([df.iloc[ids1].reset_index(drop=True),\n",
    "                                         df.iloc[ids2].reset_index(drop=True)], \n",
    "                                        axis = 1)\n",
    "                match_to_add.columns = ['id_1','name_1','latitude_1','longitude_1','categories_1',\n",
    "                                        'id_2','name_2','latitude_2','longitude_2', 'categories_2']\n",
    "                \n",
    "                ids1 = []\n",
    "                ids2 = []\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'{count} pairs generated. Time elapsed: {elapsed}')\n",
    "                yield match_to_add\n",
    "                start_time = time.time()\n",
    "                count = 0\n",
    "                del match_to_add\n",
    "    if count > 0:\n",
    "        match_to_add = pd.concat([df.iloc[ids1].reset_index(drop=True),\n",
    "                                         df.iloc[ids2].reset_index(drop=True)], \n",
    "                                        axis = 1)\n",
    "        match_to_add.columns = ['id_1','name_1','latitude_1','longitude_1','categories_1',\n",
    "                                'id_2','name_2','latitude_2','longitude_2', 'categories_2']\n",
    "        del ids1, ids2\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'{count} pairs generated. Time elapsed: {elapsed}')\n",
    "        yield match_to_add\n",
    "\n",
    "\n",
    "def generateSubmission(id_dict):\n",
    "    for entry_id in id_dict:\n",
    "        neighbours = id_dict[entry_id].neighbours\n",
    "        matches_to_add = []\n",
    "        for ind in neighbours:\n",
    "            if neighbours[ind] > CFG.th:\n",
    "                matches_to_add.append(ind)\n",
    "\n",
    "        id_dict[entry_id] = ' '.join(matches_to_add)\n",
    "\n",
    "\n",
    "    result = pd.DataFrame.from_dict(id_dict, orient = 'index', columns = ['match'])\n",
    "    result.index.name = 'id'\n",
    "    result = result.reset_index()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 237225.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38882 pairs ids generated\n",
      "38882 pairs generated. Time elapsed: 0.03725099563598633\n",
      "EVAL: [0/78] Elapsed 0m 5s (remain 6m 43s) \n",
      "EVAL: [77/78] Elapsed 4m 4s (remain 0m 0s) \n",
      "time: 245s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 38882/38882 [00:04<00:00, 8491.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID dict generated. Time elapsed: 5e+00 s\n",
      "Post processing finished. Time elapsed: 0.37008166313171387\n",
      "IOU score:  0.8596932123432113\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #initializing dict for id: poi, for poi:{id1,id2,..etc}, generate pairs with KNN\n",
    "    id_dict = {str(entry_id) : EntryID(str(entry_id)) for entry_id in init_data['id'].values}\n",
    "    pair_ids = generatePairsIds(init_data)\n",
    "    \n",
    "    for test_pairs in pairsGenerator(init_data, pair_ids):\n",
    "        \n",
    "        # take pairs from generator and scale their longitude and latitude\n",
    "        scaler=load(CFG.scaler_2)\n",
    "        num_features = [['latitude_1','longitude_1'],['latitude_2','longitude_2']]\n",
    "        test_pairs[num_features[0]] = scaler.transform(test_pairs[num_features[0]])\n",
    "        test_pairs[num_features[1]] = scaler.transform(test_pairs[num_features[1]])\n",
    "        \n",
    "        # get euclidian distance as feature\n",
    "        test_pairs['distance'] = test_pairs.apply(lambda x: euclidianDistance(x.latitude_1,x.longitude_1,x.latitude_2,x.longitude_2), axis = 1)\n",
    "        test_pairs = test_pairs.drop(columns = ['latitude_1','longitude_1','latitude_2','longitude_2'])\n",
    "        \n",
    "        # get predictions\n",
    "        results = inferenceMain(test_pairs)\n",
    "        test_pairs['match'] = results\n",
    "        \n",
    "        #fill dict for ids and pois\n",
    "        id_dict = fillIdDict(test_pairs, id_dict)\n",
    "    \n",
    "    # process answers\n",
    "    id_dict = postProcess(id_dict, th = 0.9)\n",
    "    submission = generateSubmission(id_dict)\n",
    "    score = intersectionOverUnion(train_scoring_dict,submission)\n",
    "    print('IOU score: ', score)\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
